{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a1fabf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da31729",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e85b1938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = os.getenv('HOME') + '/aiffel/Aiffel_Online5/Going-depper06/data/ChatbotData.csv'\n",
    "csv = pd.read_csv(csv_path)\n",
    "csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "02824de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = csv['Q']\n",
    "answers = csv['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1a090137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:12시 땡!\n",
      "A:하루가 또 가네요.\n",
      "Q:1지망 학교 떨어졌어\n",
      "A:위로해 드립니다.\n",
      "Q:3박4일 놀러가고 싶다\n",
      "A:여행은 언제나 좋죠.\n",
      "Q:3박4일 정도 놀러가고 싶다\n",
      "A:여행은 언제나 좋죠.\n",
      "Q:PPL 심하네\n",
      "A:눈살이 찌푸려지죠.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'Q:{questions[i]}')\n",
    "    print(f'A:{answers[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279bbef5",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28574f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower() # 소문자 변환\n",
    "    sentence = re.sub(r\"[^a-zA-Z가-힣0-9?.!,]+\", \" \", sentence) # 영어, 한글, 숫자, 특수문자 제외하고 제거\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7cae63",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b14bef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a26cefa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(src_data, tgt_data): # 소스 문장 데이터와 타켓 문장 데이터 입력으로 받음\n",
    "    mecab_src_corpus, mecab_tgt_corpus = [], []\n",
    "    mecab_src_len_list, mecab_tgt_len_list = [], []\n",
    "    \n",
    "    for s, t in zip(src_data, tgt_data): # preprocess_sentence()로 정제, 토큰화\n",
    "        s = mecab.morphs(preprocess_sentence(s))\n",
    "        t = mecab.morphs(preprocess_sentence(t))\n",
    "        \n",
    "        mecab_src_corpus.append(s)\n",
    "        mecab_tgt_corpus.append(t)\n",
    "        \n",
    "        mecab_src_len_list.append(len(s))\n",
    "        mecab_tgt_len_list.append(len(t))\n",
    "        \n",
    "    mecab_num_tokens = mecab_src_len_list + mecab_tgt_len_list\n",
    "    \n",
    "    mean_len = np.mean(mecab_num_tokens)\n",
    "    max_len = np.max(mecab_num_tokens)\n",
    "    mid_len = np.median([mean_len, max_len]) # 토큰 길이 중간값\n",
    "    \n",
    "    # mid_len 이상인 문장 제외\n",
    "    src_corpus, tgt_corpus = [], []\n",
    "    for q, a in zip(mecab_src_corpus, mecab_tgt_corpus):\n",
    "        if len(q) <= mid_len and len(a) <= mid_len:\n",
    "            if q not in src_corpus and a not in tgt_corpus: # 중복 검사\n",
    "                src_corpus.append(q)\n",
    "                tgt_corpus.append(a)\n",
    "    \n",
    "    return src_corpus, tgt_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf626051",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_corpus, ans_corpus = build_corpus(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9cb12f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12', '시', '땡', '!'] ['하루', '가', '또', '가', '네요', '.']\n"
     ]
    }
   ],
   "source": [
    "print(que_corpus[0], ans_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2bae40",
   "metadata": {},
   "source": [
    "## Step 4. Agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f497be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_path = os.getenv('HOME') + '/aiffel/Aiffel_Online5/Going-depper06/model/ko.bin'\n",
    "w2v = gensim.models.Word2Vec.load(w2v_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10c2880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(sentence, word2vec):\n",
    "    try:\n",
    "        _from = random.choice(sentence)\n",
    "        _to = w2v.most_similar(_from)[0][0]\n",
    "    except:\n",
    "        return sentence\n",
    "    \n",
    "    res = []\n",
    "    for x in sentence:\n",
    "        if x is _from: res.append(_to)\n",
    "        else: res.append(x)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2206498e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_341/3786821894.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  _to = w2v.most_similar(_from)[0][0]\n"
     ]
    }
   ],
   "source": [
    "arg_que_corpus = [lexical_sub(x, w2v) for x in que_corpus]\n",
    "arg_ans_corpus = [lexical_sub(x, w2v) for x in ans_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "244ba677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q : ['12', '시', '땡', '!'] ['12', '시', '땡', '!']\n",
      "A : ['<start>', '하루', '가', '또', '가', '네요', '.', '<end>'] ['하루', '가', '또', '가', '군요', '.']\n",
      "Q : ['1', '지망', '학교', '떨어졌', '어'] ['1', '지망', '학교의', '떨어졌', '어']\n",
      "A : ['<start>', '위로', '해', '드립니다', '.', '<end>'] ['위로', '해', '드립니다', '는데']\n",
      "Q : ['3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다'] ['3', '박', '4', '일', '살', '러', '가', '고', '싶', '다']\n",
      "A : ['<start>', '여행', '은', '언제나', '좋', '죠', '.', '<end>'] ['여행', '은', '언제나', '좋', '죠', '는데']\n",
      "Q : ['ppl', '심하', '네'] ['ppl', '심하', '네']\n",
      "A : ['<start>', '눈살', '이', '찌푸려', '지', '죠', '.', '<end>'] ['눈살', '이', '찌푸려', '지', '죠', '.']\n",
      "Q : ['sd', '카드', '망가졌', '어'] ['sd', '단말기', '망가졌', '어']\n",
      "A : ['<start>', '다시', '새로', '사', '는', '게', '마음', '편해요', '.', '<end>'] ['다시', '새로', '타', '는', '게', '마음', '편해요', '.']\n",
      "Q : ['sns', '맞', '팔', '왜', '안', '하', '지'] ['sns', '맞', '팔', '과연', '안', '하', '지']\n",
      "A : ['<start>', '잘', '모르', '고', '있', '을', '수', '도', '있', '어요', '.', '<end>'] ['잘', '모르', '고', '있', '을', '수', '때문', '있', '어요', '.']\n",
      "Q : ['sns', '시간', '낭비', '인', '거', '아', '는데', '매일', '하', '는', '중'] ['sns', '시간', '낭비', '인은', '거', '아', '는데', '매일', '하', '는', '중']\n",
      "A : ['<start>', '시간', '을', '정하', '고', '해', '보', '세요', '.', '<end>'] ['시간', '을', '정하', '고', '해의', '보', '세요', '.']\n",
      "Q : ['sns', '보', '면', '나', '만', '빼', '고', '다', '행복', '해', '보여'] ['sns', '보', '면', '나', '만', '빼', '고', '다', '행복', '해의', '보여']\n",
      "A : ['<start>', '자랑', '하', '는', '자리', '니까요', '.', '<end>'] ['자랑', '하', '는', '자리', 'ㄴ데요', '.']\n",
      "Q : ['가끔', '궁금', '해'] ['가끔', '궁금하', '해']\n",
      "A : ['<start>', '그', '사람', '도', '그럴', '거', '예요', '.', '<end>'] ['그', '사람', '도', '그럴', '것', '예요', '.']\n",
      "Q : ['가끔', '은', '혼자', '인', '게', '좋', '다'] ['가끔', '은', '혼자', '인', '게', '좋', '으며']\n",
      "A : ['<start>', '혼자', '를', '즐기', '세요', '.', '<end>'] ['혼자', '를', '좋아하', '세요', '.']\n",
      "Q : ['가난', '한', '자', '의', '설움'] ['가난', '한', '자마자', '의', '설움']\n",
      "A : ['<start>', '돈', '은', '다시', '들어올', '거', '예요', '.', '<end>'] ['돈', '은', '곧바로', '들어올', '거', '예요', '.']\n",
      "Q : ['가만', '있', '어도', '땀', '난다'] ['가만', '있', '더라도', '땀', '난다']\n",
      "A : ['<start>', '땀', '을', '식혀', '주', '세요', '.', '<end>'] ['땀', '를', '식혀', '주', '세요', '.']\n",
      "Q : ['가상', '화폐', '쫄딱', '망함'] ['가상', '화폐', '쫄딱', '망함']\n",
      "A : ['<start>', '어서', '잊', '고', '새', '출발', '하', '세요', '.', '<end>'] ['어서', '잊어버리', '고', '새', '출발', '하', '세요', '.']\n",
      "Q : ['가스', '불', '켜', '고', '나갔', '어'] ['가스', '불이', '켜', '고', '나갔', '어']\n",
      "A : ['<start>', '빨리', '집', '에', '돌아가', '서', '끄', '고', '나오', '세요', '.', '<end>'] ['빨리', '집', '에', '돌아가', '서', '끄', '고', '나오', '세요', '는데']\n",
      "Q : ['가스', '비', '너무', '많이', '나왔', '다', '.'] ['증기', '비', '너무', '많이', '나왔', '다', '.']\n",
      "A : ['<start>', '다음', '달', '에', '는', '더', '절약', '해', '봐요', '.', '<end>'] ['다음', '달', '에', '는', '더', '절약', '해의', '봐요', '.']\n",
      "Q : ['가스', '비', '비싼데', '감기', '걸리', '겠', '어'] ['가스', '비', '비싼데', '감기', '매달리', '겠', '어']\n",
      "A : ['<start>', '따뜻', '하', '게', '사세요', '!', '<end>'] ['따뜻', '하', '도록', '사세요', '!']\n",
      "Q : ['가족', '여행', '가', '기', '로', '했', '어'] ['가족', '여행', '가', '기', '로', '했', '어서']\n",
      "A : ['<start>', '온', '가족', '이', '모두', '마음', '에', '드', '는', '곳', '으로', '가', '보', '세요', '.', '<end>'] ['온', '가족', '이', '모두', '마음', '에', '드', '는', '곳', '으로', '가', '보', 'ㅂ시오', '.']\n",
      "Q : ['가족', '끼리', '여행', '간다', '.'] ['친지', '끼리', '여행', '간다', '.']\n",
      "A : ['<start>', '더', '가까워질', '기회', '가', '되', '겠', '네요', '.', '<end>'] ['더', '가까워질', '일자리', '가', '되', '겠', '네요', '.']\n",
      "Q : ['가족', '들', '보', '고', '싶', '어'] ['가족', '들', '보', '고', '싶', '어서']\n",
      "A : ['<start>', '저', '도', '요', '.', '<end>'] ['저', '도', '요', '는데']\n",
      "Q : ['가족', '들', '이랑', '서먹', '해'] ['가족', '들', '아줌마', '서먹', '해']\n",
      "A : ['<start>', '다', '들', '바빠서', '이야기', '할', '시간', '이', '부족', '했', '나', '봐요', '.', '<end>'] ['다', '부유층', '바빠서', '이야기', '할', '시간', '이', '부족', '했', '나', '봐요', '.']\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(f\"Q : {que_corpus[i]} {arg_que_corpus[i]}\")\n",
    "    print(f\"A : {ans_corpus[i]} {arg_ans_corpus[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0b5becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_corpus = que_corpus + arg_que_corpus + que_corpus\n",
    "ans_corpus = ans_corpus + ans_corpus + arg_ans_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62dd129",
   "metadata": {},
   "source": [
    "## Step 5. 데이터 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a97dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  타겟 데이터에 <start>, <end> 토큰 추가\n",
    "ans_corpus = [[\"<start>\"] + ans + [\"<end>\"] for ans in ans_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2f24fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>', '하루', '가', '또', '가', '네요', '.', '<end>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "512e934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터에 대한 단어 사전 구축, 벡터화\n",
    "data = que_corpus + ans_corpus\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=None, filters=' ', oov_token='<unk>')\n",
    "tokenizer.fit_on_texts(data)\n",
    "tensor = tokenizer.texts_to_sequences(data)\n",
    "tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6c69590",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(tokenizer.index_word) + 2 # 특수 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d725077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45822, 25)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21df2a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, dec_train = tensor[:22911], tensor[22911:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db72e853",
   "metadata": {},
   "source": [
    "## Step 6. 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af56b71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5e86560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e5e8d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "412e0015",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e971b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2eb05ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        # Q, K, V 순서에 주의하세요!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9295269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c44e86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a397fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64f4bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eae5697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(512)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ab910ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e04c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5bd54e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 6\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "dropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d9431e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 358/358 [01:17<00:00,  4.64it/s, Loss 5.8904] \n",
      "Epoch  2: 100%|██████████| 358/358 [01:04<00:00,  5.51it/s, Loss 3.6026]\n",
      "Epoch  3: 100%|██████████| 358/358 [01:04<00:00,  5.55it/s, Loss 2.1734]\n",
      "Epoch  4: 100%|██████████| 358/358 [01:04<00:00,  5.53it/s, Loss 1.1949]\n",
      "Epoch  5: 100%|██████████| 358/358 [01:04<00:00,  5.56it/s, Loss 0.9051]\n",
      "Epoch  6: 100%|██████████| 358/358 [01:04<00:00,  5.53it/s, Loss 0.7909]\n",
      "Epoch  7: 100%|██████████| 358/358 [01:04<00:00,  5.55it/s, Loss 0.7366]\n",
      "Epoch  8: 100%|██████████| 358/358 [01:04<00:00,  5.56it/s, Loss 0.7079]\n",
      "Epoch  9: 100%|██████████| 358/358 [01:04<00:00,  5.56it/s, Loss 0.6954]\n",
      "Epoch 10: 100%|██████████| 358/358 [01:04<00:00,  5.59it/s, Loss 0.6733]\n",
      "Epoch 11: 100%|██████████| 358/358 [01:04<00:00,  5.59it/s, Loss 0.6881]\n",
      "Epoch 12: 100%|██████████| 358/358 [01:04<00:00,  5.59it/s, Loss 0.6548]\n",
      "Epoch 13: 100%|██████████| 358/358 [01:03<00:00,  5.61it/s, Loss 0.5691]\n",
      "Epoch 14: 100%|██████████| 358/358 [01:03<00:00,  5.61it/s, Loss 0.5070]\n",
      "Epoch 15: 100%|██████████| 358/358 [01:03<00:00,  5.61it/s, Loss 0.4647]\n",
      "Epoch 16: 100%|██████████| 358/358 [01:03<00:00,  5.64it/s, Loss 0.4236]\n",
      "Epoch 17: 100%|██████████| 358/358 [01:03<00:00,  5.62it/s, Loss 0.3911]\n",
      "Epoch 18: 100%|██████████| 358/358 [01:03<00:00,  5.65it/s, Loss 0.3789]\n",
      "Epoch 19: 100%|██████████| 358/358 [01:03<00:00,  5.63it/s, Loss 0.3457]\n",
      "Epoch 20: 100%|██████████| 358/358 [01:03<00:00,  5.64it/s, Loss 0.3217]\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=n_layers,\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    d_ff=d_ff,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=dropout,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                                                     dec_train[idx:idx+BATCH_SIZE],\n",
    "                                                                     transformer,\n",
    "                                                                     optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3995837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, model, tokenizer):\n",
    "        mecab = Mecab()\n",
    "        sentence = mecab.morphs(preprocess_sentence(sentence))\n",
    "        sentence = tokenizer.texts_to_sequences(sentence)\n",
    "        _input = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            sentence, maxlen=enc_train.shape[-1], padding='post')\n",
    "\n",
    "        ids = []\n",
    "        output = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "        \n",
    "        for i in range(dec_train.shape[-1]):\n",
    "            enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(_input, output)\n",
    "\n",
    "            predictions, enc_attns, dec_attns, dec_enc_attns = model(\n",
    "                _input, output, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "            \n",
    "            predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "            \n",
    "            if tokenizer.word_index['<end>'] == predicted_id:\n",
    "                result = ' '.join(tokenizer.sequences_to_texts([ids]))\n",
    "                return result, enc_attns, dec_attns, dec_enc_attns\n",
    "            \n",
    "            ids.append(predicted_id)\n",
    "            output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "    \n",
    "        result = ' '.join(self.tokenizer.sequences_to_texts([ids]))\n",
    "        return result, enc_attns, dec_attns, dec_enc_attns\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5d37255",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    \"지루하다, 놀러가고 싶어.\",\n",
    "    \"오늘 일찍 일어났더니 피곤하다.\",\n",
    "    \"간만에 여자친구랑 데이트 하기로 했어.\",\n",
    "    \"집에 있는다는 소리야.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da442324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu_single(model, src_sentence, tgt_sentence, tokenizer):\n",
    "    src_tokens = tokenizer.texts_to_sequences(src_sentence)\n",
    "    tgt_tokens = tokenizer.texts_to_sequences(tgt_sentence)\n",
    "\n",
    "    if (len(src_tokens) > enc_train.shape[-1]): return None\n",
    "    if (len(tgt_tokens) > enc_train.shape[-1]): return None\n",
    "\n",
    "    reference = tgt_sentence.split()\n",
    "    candidate, _, _, _ = evaluate(src_sentence, model, tokenizer)\n",
    "    candidate = candidate.split()\n",
    "\n",
    "    score = sentence_bleu([reference], candidate,\n",
    "                          smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    if True:\n",
    "        print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", candidate)\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d7593a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(model, tokenizer, src_sentences, tgt_sentence):\n",
    "        total_score = 0.0\n",
    "        sample_size = len(src_sentences)\n",
    "\n",
    "        for idx in tqdm(range(sample_size)):\n",
    "            score = eval_bleu_single(model, src_sentences[idx], tgt_sentence[idx], tokenizer)\n",
    "            if not score: continue\n",
    "\n",
    "            total_score += score\n",
    "\n",
    "        print(\"Num of Sample:\", sample_size)\n",
    "        print(\"Total Score:\", total_score / sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d69ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(samples, model, tokenizer):\n",
    "        print('# 제출')\n",
    "        print()\n",
    "        \n",
    "        print('Translations')\n",
    "        for s in samples:\n",
    "            result,_,_,_ = evaluate(s, model, tokenizer)\n",
    "            print(f'> {i + 1}')\n",
    "            print(f'Q: {s}')\n",
    "            print(f'A: {result}')\n",
    "            print()\n",
    "            \n",
    "        print('Hyperparameters')\n",
    "        print(f'> n_layers: {n_layers}')\n",
    "        print(f'> d_model: {d_model}')\n",
    "        print(f'> n_heads: {n_heads}')\n",
    "        print(f'> d_ff: {d_ff}')\n",
    "        print(f'> dropout: {dropout}')\n",
    "        print()\n",
    "        \n",
    "        print('Training Parameters')\n",
    "        print(f'> Batch Size: {BATCH_SIZE}')\n",
    "        print(f'> Epoch At: {EPOCHS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65ae7286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 제출\n",
      "\n",
      "Translations\n",
      "> 5\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 다른 도 곳 을 떠나 보 세요 .\n",
      "\n",
      "> 5\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 오늘 은 힘내 려 하 지 않 아요 .\n",
      "\n",
      "> 5\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 휴식 도 필요 해요 .\n",
      "\n",
      "> 5\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 같이 해 보 세요 .\n",
      "\n",
      "Hyperparameters\n",
      "> n_layers: 6\n",
      "> d_model: 512\n",
      "> n_heads: 8\n",
      "> d_ff: 2048\n",
      "> dropout: 0.3\n",
      "\n",
      "Training Parameters\n",
      "> Batch Size: 64\n",
      "> Epoch At: 20\n"
     ]
    }
   ],
   "source": [
    "submit(samples, transformer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b9c10e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_questions = questions[:10]  \n",
    "sample_answers = answers[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cfcfea76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:03,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  12시 땡!\n",
      "Model Prediction:  ['기분', '.']\n",
      "Real:  ['하루가', '또', '가네요.']\n",
      "Score: 0.000000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:01<00:06,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  1지망 학교 떨어졌어\n",
      "Model Prediction:  ['아직', '은', '매력', '이', '에요', '.']\n",
      "Real:  ['위로해', '드립니다.']\n",
      "Score: 0.000000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:02<00:06,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  3박4일 놀러가고 싶다\n",
      "Model Prediction:  ['어느덧', '3', '주', '가', '어느덧', '.']\n",
      "Real:  ['여행은', '언제나', '좋죠.']\n",
      "Score: 0.000000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:03<00:05,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  3박4일 정도 놀러가고 싶다\n",
      "Model Prediction:  ['어느덧', '3', '주', '가', '어느덧', '.']\n",
      "Real:  ['여행은', '언제나', '좋죠.']\n",
      "Score: 0.000000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:04<00:04,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  PPL 심하네\n",
      "Model Prediction:  ['약', '을', '발라', '드릴게요', '.']\n",
      "Real:  ['눈살이', '찌푸려지죠.']\n",
      "Score: 0.000000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:05<00:03,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  SD카드 망가졌어\n",
      "Model Prediction:  ['다시', '새로', '시작', '하', '세요', '.']\n",
      "Real:  ['다시', '새로', '사는', '게', '마음', '편해요.']\n",
      "Score: 0.086334\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:06<00:03,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  SD카드 안돼\n",
      "Model Prediction:  ['다시', '새로', '시작', '하', '세요', '.']\n",
      "Real:  ['다시', '새로', '사는', '게', '마음', '편해요.']\n",
      "Score: 0.086334\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:07<00:02,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  SNS 맞팔 왜 안하지ㅠㅠ\n",
      "Model Prediction:  ['싫', '지', '않', '으면', '만나', '야죠', '.']\n",
      "Real:  ['잘', '모르고', '있을', '수도', '있어요.']\n",
      "Score: 0.000000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:09<00:01,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  SNS 시간낭비인 거 아는데 매일 하는 중\n",
      "Model Prediction:  ['싫', '지', '않', '으면', '만나', '야죠', '.']\n",
      "Real:  ['시간을', '정하고', '해보세요.']\n",
      "Score: 0.000000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  SNS 시간낭비인데 자꾸 보게됨\n",
      "Model Prediction:  ['싫', '지', '않', '으면', '만나', '야죠', '.']\n",
      "Real:  ['시간을', '정하고', '해보세요.']\n",
      "Score: 0.000000\n",
      "\n",
      "Num of Sample: 10\n",
      "Total Score: 0.017266800427409006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "eval_bleu(transformer, tokenizer, sample_questions, sample_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f58a5e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:집에 있을래 \n",
      " A:같이 해 보 세요 .\n"
     ]
    }
   ],
   "source": [
    "s = '집에 있을래'\n",
    "result,_,_,_ = evaluate(s, transformer, tokenizer)\n",
    "print(f'Q:{s} \\n A:{result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "decb6a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:집을 나갈 거야 \n",
      " A:같이 해 보 세요 .\n"
     ]
    }
   ],
   "source": [
    "s = '집을 나갈 거야'\n",
    "result,_,_,_ = evaluate(s, transformer, tokenizer)\n",
    "print(f'Q:{s} \\n A:{result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0406eb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:집은 비어 있어 \n",
      " A:같이 해 보 세요 .\n"
     ]
    }
   ],
   "source": [
    "s = '집은 비어 있어'\n",
    "result,_,_,_ = evaluate(s, transformer, tokenizer)\n",
    "print(f'Q:{s} \\n A:{result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af385090",
   "metadata": {},
   "source": [
    "## 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf14ad2d",
   "metadata": {},
   "source": [
    "### 루브릭"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887ddcf5",
   "metadata": {},
   "source": [
    "|평가문항|상세기준|\n",
    "|:---|:---|\n",
    "|1. 챗봇 훈련데이터 전처리 과정이 체계적으로 진행되었는가?|챗봇 훈련데이터를 위한 전처리와 augmentation이 적절히 수행되어 3만개 가량의 훈련데이터셋이 구축되었다.|\n",
    "|2. transformer 모델을 활용한 챗봇 모델이 과적합을 피해 안정적으로 훈련되었는가?|과적합을 피할 수 있는 하이퍼파라미터 셋이 적절히 제시되었다.|\n",
    "|3. 챗봇이 사용자의 질문에 그럴듯한 형태로 답하는 사례가 있는가?|주어진 예문을 포함하여 챗봇에 던진 질문에 적절히 답하는 사례가 제출되었다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b229b2",
   "metadata": {},
   "source": [
    "### 고찰\n",
    "> result1  \n",
    "Q: 지루하다, 놀러가고 싶어.  \n",
    "A: 다른 도 곳 을 떠나 보 세요 .\n",
    "\n",
    "> result2  \n",
    "Q: 오늘 일찍 일어났더니 피곤하다.  \n",
    "A: 오늘 은 힘내 려 하 지 않 아요 .\n",
    "\n",
    "> result3  \n",
    "Q: 간만에 여자친구랑 데이트 하기로 했어.  \n",
    "A: 휴식 도 필요 해요 .\n",
    "\n",
    "> result4  \n",
    "Q: 집에 있는다는 소리야.  \n",
    "A: 같이 해 보 세요 .\n",
    "\n",
    "**4번**을 제외하고는 전체적으로 완전히 딴 소리를 하는 것 같지는 않았지만 그렇다고 질문을 완전히 파악한 것 같지도 않았다. **1번**같은 경우에는 조사가 어색한 것을 제외하면 `지루하다, 놀러가고 싶어.` 라는 문장에 `다른 도 곳 을 떠나 보 세요` 라고 적절히 답했고, **4번** 같은 경우에는 질문과 관계없는 답을 했다. 하이퍼 파라미터의 조정이 있었는데도 가장 잘 나온 결과가 기대치에 미치지 못한 이유를 찾아봤을 때, Augmentation 과정에서 `A : ['<start>', '돈', '은', '다시', '들어올', '거', '예요', '.', '<end>'] -> ['돈', '은', '곧바로', '들어올', '거', '예요', '.']`를 예로 들어, 의미가 약간 달라진 문장이 생성되었을 것이라고 추측할 수 있었고 이것이 영향을 미쳤을 수도 있었을 것 같다.  \n",
    "제대로 답하지 못한 **4번** 대답에 대해 `집`이라는 키워드를 고정하고 임의의 질문을 했을 때 다음과 같이 답했다.\n",
    "> result4-1  \n",
    "Q: **집**에 있을래  \n",
    "A:같이 해 보 세요 .  \n",
    "\n",
    ">result4-2  \n",
    "Q: **집**을 나갈 거야  \n",
    "A:같이 해 보 세요 .  \n",
    "\n",
    ">result4-3  \n",
    "Q: **집**은 비어 있어  \n",
    "A:같이 해 보 세요 .  \n",
    "\n",
    "세 가지 질문에 모두 동일하게 답한 것으로 보아 `집`이라는 키워드에 같은 대답이 나올 확률이 높음을 알 수 있다. augmentation을 적용하였으나 기본적으로 훈련시킨 데이터의 양이 적고, augmentation은 기존(이번 케이스의 경우 길지 않은) 문장의 단어를 약간 수정하여 데이터의 양을 늘렸기 때문에 해당 키워드에 대한 반응이 고정되었을 수도 있을 것 같다는 생각이 들었다. 이 문제에 대해서는 모델의 매커니즘을 확실히 이해하고 데이터셋이 미치는 영향을 더 조사할 필요가 있겠다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82322d94",
   "metadata": {},
   "source": [
    "### 회고\n",
    "|KPT|내용|\n",
    "|:---|:---|\n",
    "|Keep|평가 기준에 맞추어 과제를 수행하고 결과를 도출하였다.|\n",
    "|Problem|실험 과정을 기록하지 못했고 하이퍼파라미터의 조정 과정도 체계적이지 못했다.|\n",
    "|Try|다양한 조건 하에서 실험할 때에는 상관관계를 파악하기 쉽도록 적절히 통제하고 기록하도록 한다.|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

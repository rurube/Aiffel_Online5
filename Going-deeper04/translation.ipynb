{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Seq2seq으로 번역기 만들기 [프로젝트]\n",
    "프로젝트: 한영 번역기 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/korean-english-park.train.ko', 'r') as f:\n",
    "    train_ko_data = f.read().splitlines()\n",
    "\n",
    "with open('./data/korean-english-park.train.en', 'r') as f:\n",
    "    train_en_data = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ko_data :  94123\n",
      "train_en_data :  94123\n"
     ]
    }
   ],
   "source": [
    "print('train_ko_data : ', len(train_ko_data))\n",
    "print('train_en_data : ', len(train_en_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>  개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      ">>  Much of personal computing is about \"can you top this?\" \n",
      "\n",
      ">>  제 23차 연례 컴덱스 박람회의 개회사를 한 케이츠는 2년여전 기술 산업의 거품이 붕괴된 이후에 첨단 기술에 대해 부정적인 인식이 있다고 말했다.\n",
      ">>  Gates, who opened the 23rd annual Comdex trade show, said there was a negative perception of high tech following the collapse of the tech bubble about two years ago. \n",
      "\n",
      ">>  국제 원자력 기구는 북한이 핵 무기 개발 계획을 중지하고, 즉각적으로 \"모든 관련 시설들\"을 공개하여 사찰을 받으라고 요구했다.\n",
      ">>  The International Atomic Energy Agency called on North Korea to end any nuclear weapon program and open \"all relevant facilities\" to inspections immediately. \n",
      "\n",
      ">>  일본에서 124명의 사망자를 낸 폐암 치료제\n",
      ">>  Microsoft had \"leveraged its PC monopoly in which it is unfairly advantaged.\" \n",
      "\n",
      ">>  그러나 미국은 개개의 시험과 성인의 식자율(識字率)에서 하위에 머물렀다.\n",
      ">>  The United States, however, finished low in each test and in adult literacy. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,500,100):\n",
    "    print('>> ', train_ko_data[i])\n",
    "    print('>> ', train_en_data[i], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set을 이용한 중복 제거\n",
    "cleaned_corpus = list(set(zip(train_ko_data, train_en_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78968"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>  모든 혐의들이 인정될 경우 러시아 언론에 의해 ‘비트세브스키 미치광이’로 불리는 피추슈킨은 지난 1992년 52명을 살해한 안드레이 치카틸로 이후 최악의 연쇄살인범으로 기록될 전망이다.\n",
      ">>  If convicted, Pichushkin called the \"Bitsevsky Maniac\" by Russian media after the Moscow park where many of the alleged victims were killed would be Russia's most deadly serial killer since Andrei Chikatilo, convicted in 1992 of 52 murders. \n",
      "\n",
      ">>  그린피스와 다른 환경단체는 중국의 환경 정책을 비판해 왔다.\n",
      ">>  Greenpeace and other environmental activists have criticized China for its environmental policies. \n",
      "\n",
      ">>  그는 선행을 할 것, 힘과 더불어 두뇌에 의존할 것, 공평하게 통치할 것, 계획을 잘 세울 것, 사람들의 비밀을 지키고 타인의 실수에서 교훈을 얻을 것을 요구한다.\n",
      ">>  He calls for doing good, depending on brains as well as *brawn, ruling fairly, planning well, keeping people’s secrets and learning from others’ mistakes. \n",
      "\n",
      ">>  이어 “진심으로 대화하고 마음 깊이 변화하면 모든 악의 원인을 이겨 낼 수 있다”고 덧붙였다.\n",
      ">>  \"Only through the conversion of hearts, only through a change in the depths of our hearts can the cause of all this evil be overcome.\" \n",
      "\n",
      ">>  한편 누르 하산 후세인 소말리아 총리는 이번 교전사태에 대해 공식 언급을 피했다.\n",
      ">>  Somali Prime Minister Nur Hassan Nur Ade had no immediate comment on the latest fighting, the latest in a lengthy insurgency against his government and its Ethiopian backers. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,500,100):\n",
    "    print('>> ', cleaned_corpus[i][0])\n",
    "    print('>> ', cleaned_corpus[i][1], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 전처리\n",
    "# 정규식 변경, mecab 사용\n",
    "def preprocess_sentence_ko(sentence):\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^가-힣?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    mecab = Mecab()\n",
    "    sentence = mecab.morphs(sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 전처리\n",
    "def preprocess_sentence_en(sentence, s_token=False, e_token=False):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "    \n",
    "    sentence = sentence.split()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 길이가 40 이하인 데이터 선별\n",
    "kor_corpus = []\n",
    "eng_corpus = []\n",
    "\n",
    "for x in cleaned_corpus:\n",
    "    ko = preprocess_sentence_ko(x[0])\n",
    "    en = preprocess_sentence_en(x[1], s_token=True, e_token=True)\n",
    "    \n",
    "    if len(ko) <= 40 and len(en) <= 40:\n",
    "        kor_corpus.append(ko)\n",
    "        eng_corpus.append(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63139 63139\n",
      ">>  ['레바논', '에서', '의', '폭발', '에', '유엔', '병사', '들', '사망', '.']\n",
      ">>  ['<start>', 'authorities', 'have', 'said', 'that', 'fatah', 'al', 'islam', 'militants', 'who', 'have', 'been', 'arrested', 'and', 'interrogated', 'have', 'confessed', 'there', 'was', 'a', 'plan', 'to', 'attack', 'the', 'un', '.', '<end>'] \n",
      "\n",
      ">>  ['스리랑카', '당국', '은', '열차', '가', '콜롬보', '시', '중심가', '에', '있', '는', '스', '링', '랑', '카', '의', '주요', '역사', '인', '포트', '역', '에', '열차', '가', '들어설', '때', '자살', '폭탄', '테러', '가', '발생', '했', '다고', '밝혔', '다', '.']\n",
      ">>  ['<start>', 'the', 'suicide', 'bombing', 'took', 'place', 'as', 'a', 'train', 'was', 'pulling', 'into', 'fort', 'station', 'the', 'country', 's', 'main', 'railway', 'station', 'in', 'colombo', 's', 'city', 'center', ',', 'authorities', 'said', '.', '<end>'] \n",
      "\n",
      ">>  ['게임', '은', '연령', '에', '관련', '된', '인지', '기능', '감퇴', '와', '시각', '적', '기민성', '을', '개선', '하', '도록', '고안', '됐', '다', '.']\n",
      ">>  ['<start>', 'they', 're', 'designed', 'to', 'reverse', 'age', 'related', 'cognitive', 'decline', 'and', 'improve', 'visual', 'alertness', '.', '<end>'] \n",
      "\n",
      ">>  ['이런', '가운데', '이스라엘', '고위', '지휘관', '은', '레바논', '국경', '지대', '의', '경계', '를', '강화', '할', '것', '을', '이스라엘', '군', '에', '지시', '했', '다', '.']\n",
      ">>  ['<start>', 'israel', 's', 'military', 'chief', 'thursday', 'ordered', 'his', 'forces', 'to', 'take', 'the', 'necessary', 'steps', 'to', 'protect', 'the', 'country', 's', 'border', 'with', 'lebanon', '.', '<end>'] \n",
      "\n",
      ">>  ['이', '와', '관련', '부시', '는', '중국', '방문기', '간', '동안', '올림픽', '개막식', '에', '참석', '한', '뒤', '후진타오', '중국', '국가', '주석', '과', '회담', '할', '예정', '이', '라고', '밝혔', '다', '.']\n",
      ">>  ['<start>', 'president', 'bush', 'has', 'said', 'he', 'intends', 'to', 'meet', 'china', 's', 'president', 'during', 'a', 'trip', 'to', 'see', 'the', 'olympics', '.', '<end>'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(kor_corpus), len(eng_corpus))\n",
    "\n",
    "for i in range(0,500,100):\n",
    "    print('>> ', kor_corpus[i])\n",
    "    print('>> ', eng_corpus[i], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어수 15000으로 설정, 텐서 변환\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=15000, filters='', oov_token=\"<unk>\")\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_tensor, enc_tokenizer = tokenize(kor_corpus)\n",
    "dec_tensor, dec_tokenizer = tokenize(eng_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37077, 37077)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc_tokenizer.index_word), len(enc_tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 712,   17,    7,  389,    9,  257, 1004,   16,   90,    2,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   4,  207,   27,   13,   16, 1982,  179, 2317,  393,   40,   27,\n",
       "         39,  345,   11, 8666,   27, 4284,   73,   18,    9,  328,    7,\n",
       "        228,    2,  734,    3,    5,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_tensor[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units, return_sequences=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "\n",
    "        out, h_dec = self.gru(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (64, 40, 128)\n",
      "Decoder Output: (64, 38542)\n",
      "Decoder Hidden State: (64, 128)\n",
      "Attention: (64, 40, 1)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE     = 64\n",
    "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word) + 1 # 예: len(enc_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1 # 예: len(dec_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 128\n",
    "embedding_dim = 128\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 40\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)\n",
    "        h_dec = enc_out[:, -1]\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 987/987 [02:39<00:00,  6.18it/s, Loss 3.7526] \n",
      "Epoch  2: 100%|██████████| 987/987 [01:35<00:00, 10.37it/s, Loss 3.6977]\n",
      "Epoch  3: 100%|██████████| 987/987 [01:35<00:00, 10.36it/s, Loss 3.6976]\n",
      "Epoch  4: 100%|██████████| 987/987 [01:35<00:00, 10.31it/s, Loss 3.4117]\n",
      "Epoch  5: 100%|██████████| 987/987 [01:35<00:00, 10.28it/s, Loss 3.1013]\n",
      "Epoch  6: 100%|██████████| 987/987 [01:35<00:00, 10.30it/s, Loss 2.9338]\n",
      "Epoch  7: 100%|██████████| 987/987 [01:35<00:00, 10.32it/s, Loss 2.8035]\n",
      "Epoch  8: 100%|██████████| 987/987 [01:35<00:00, 10.31it/s, Loss 2.6999]\n",
      "Epoch  9: 100%|██████████| 987/987 [01:35<00:00, 10.31it/s, Loss 2.6153]\n",
      "Epoch 10: 100%|██████████| 987/987 [01:35<00:00, 10.31it/s, Loss 2.5451]\n",
      "Epoch 11: 100%|██████████| 987/987 [01:36<00:00, 10.26it/s, Loss 2.4837]\n",
      "Epoch 12: 100%|██████████| 987/987 [01:35<00:00, 10.29it/s, Loss 2.4282]\n",
      "Epoch 13: 100%|██████████| 987/987 [01:35<00:00, 10.30it/s, Loss 2.3780]\n",
      "Epoch 14: 100%|██████████| 987/987 [01:36<00:00, 10.25it/s, Loss 2.3314]\n",
      "Epoch 15: 100%|██████████| 987/987 [01:36<00:00, 10.21it/s, Loss 2.2878]\n",
      "Epoch 16: 100%|██████████| 987/987 [01:36<00:00, 10.26it/s, Loss 2.2466]\n",
      "Epoch 17: 100%|██████████| 987/987 [01:36<00:00, 10.26it/s, Loss 2.2093]\n",
      "Epoch 18: 100%|██████████| 987/987 [01:35<00:00, 10.29it/s, Loss 2.1712]\n",
      "Epoch 19: 100%|██████████| 987/987 [01:35<00:00, 10.31it/s, Loss 2.1360]\n",
      "Epoch 20: 100%|██████████| 987/987 [01:35<00:00, 10.30it/s, Loss 2.1033]\n",
      "Epoch 21: 100%|██████████| 987/987 [01:35<00:00, 10.31it/s, Loss 2.0706]\n",
      "Epoch 22: 100%|██████████| 987/987 [01:35<00:00, 10.32it/s, Loss 2.0401]\n",
      "Epoch 23: 100%|██████████| 987/987 [01:35<00:00, 10.31it/s, Loss 2.0116]\n",
      "Epoch 24: 100%|██████████| 987/987 [01:35<00:00, 10.31it/s, Loss 1.9833]\n",
      "Epoch 25: 100%|██████████| 987/987 [01:35<00:00, 10.31it/s, Loss 1.9556]\n",
      "Epoch 26: 100%|██████████| 987/987 [01:35<00:00, 10.31it/s, Loss 1.9307]\n",
      "Epoch 27: 100%|██████████| 987/987 [01:35<00:00, 10.32it/s, Loss 1.9045]\n",
      "Epoch 28: 100%|██████████| 987/987 [01:35<00:00, 10.32it/s, Loss 1.8812]\n",
      "Epoch 29: 100%|██████████| 987/987 [01:35<00:00, 10.31it/s, Loss 1.8576]\n",
      "Epoch 30: 100%|██████████| 987/987 [01:35<00:00, 10.31it/s, Loss 1.8370]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm    # tqdm\n",
    "import random\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_tensor.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(enc_tensor[idx:idx+BATCH_SIZE],\n",
    "                                dec_tensor[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((dec_tensor.shape[-1], enc_tensor.shape[-1]))\n",
    "    \n",
    "    sentence = preprocess_sentence_ko(sentence)\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=enc_tensor.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(dec_tensor.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result), :len(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "import matplotlib as mpl\n",
    "\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['오바마', '는', '대통령', '이', '다', '.']\n",
      "Predicted translation: obama is the president elect . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(\"오바마는 대통령이다.\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['시민', '들', '은', '도시', '속', '에', '산다', '.']\n",
      "Predicted translation: the people are not uncommon in the city . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(\"시민들은 도시 속에 산다.\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['커피', '는', '필요', '없', '다', '.']\n",
      "Predicted translation: the <unk> is not a lot of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of \n"
     ]
    }
   ],
   "source": [
    "translate(\"커피는 필요 없다.\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['일곱', '명', '의', '사망자', '가', '발생', '했', '다', '.']\n",
      "Predicted translation: two of the two people were killed . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(\"일곱 명의 사망자가 발생했다.\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 루브릭\n",
    "|평가문항|상세기준|\n",
    "|:---|:---|\n",
    "|1. 번역기 모델 학습에 필요한 텍스트 데이터 전처리가 한국어 포함하여 잘 이루어졌다.\t|구두점, 대소문자, 띄어쓰기, 한글 형태소분석 등 번역기 모델에 요구되는 전처리가 정상적으로 진행되었다.|\n",
    "|2. Attentional Seq2seq 모델이 정상적으로 구동된다.|seq2seq 모델 훈련 과정에서 training loss가 안정적으로 떨어지면서 학습이 진행됨이 확인되었다.|\n",
    "|3. 테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.|테스트용 디코더 모델이 정상적으로 만들어져서, 정답과 어느 정도 유사한 영어 번역이 진행됨을 확인하였다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 고찰\n",
    "모델이 예측한 결과를 **네이버 파파고 번역기**의 결과와 비교하였다.\n",
    "\n",
    "`input` 오바마는 대통령이다.  \n",
    "`predict` obama is the president elect.  \n",
    "`papago` Obama is the president.\n",
    "> 네 가지 에문 중 가장 잘 번역하였다. \n",
    "\n",
    "`input` 시민들은 도시 속에 산다.  \n",
    "`predict` the people are not uncommon in the city.  \n",
    "`papago` Citizens live in cities.\n",
    "> '도시'와 '사람들' 이라는 키워드는 들어갔으나, 문장의 내용이 맞지 않았다.\n",
    "\n",
    "`input` 커피는 필요 없다.  \n",
    "`predict` the \\<unk> is not a lot of the \\<unk> of the \\<unk> of the \\<unk> of the \\<unk> of the \\<unk> of the \\<unk> of the \\<unk> of the \\<unk> of the \\<unk> of the \\<unk> of the \\<unk> of  \n",
    "`papago` I don't need coffee.\n",
    "> 전처리나 모델에 문제가 있었는지 제대로 동작하지 않았다.\n",
    "\n",
    "`input` 일곱 명의 사망자가 발생했다.  \n",
    "`predict` two of the two people were killed.  \n",
    "`papago` Seven people were killed.\n",
    "> '사람들' 과 '죽음' 키워드는 포함되었으나, 세부적인 부분(인원 수)이 맞지 않았다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \\>\\>  일본에서 124명의 사망자를 낸 폐암 치료제  \n",
    "> \\>\\>  Microsoft had \"leveraged its PC monopoly in which it is unfairly advantaged.\" \n",
    "\n",
    "처음 데이터를 불러오고 출력했을 때 위와 같이 한글 문장과 영어 문장 쌍이 맞지 않는 경우가 있었다.\n",
    "\n",
    "> \\>\\>  ['레바논', '에서', '의', '폭발', '에', '유엔', '병사', '들', '사망', '.']  \n",
    "> \\>\\>  ['\\<start\\>', 'authorities', 'have', 'said', 'that', 'fatah', 'al', 'islam', 'militants', 'who', 'have', 'been', 'arrested', 'and', 'interrogated', 'have', 'confessed', 'there', 'was', 'a', 'plan', 'to', 'attack', 'the', 'un', '.', '\\<end\\>']   \n",
    "\n",
    "이런 문제가 토큰화 이후에도 보인 것으로 보아, 모델 훈련에 잘못된 데이터가 섞여있었을 것이라고 추측할 수 있다. (특히 세 번째 예시 문장에서) 이상적이지 않은 결과가 나온 것에 대해, 같은 데이터와 전처리에서 트랜스포머 등 다른 모델을 사용했을 때 결과가 어떻게 바뀔지 궁금하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 회고\n",
    "|KPT|내용|\n",
    "|:---|:---|\n",
    "|Keep|모델을 훈련시키고 예시 문장에 대한 결과를 출력하여 비교하였다.|\n",
    "|Problem|원인을 찾지 못했지만 시각화 단계에서 이상하게 출력되는 문제가 있었다.|\n",
    "|Try|시각화 오류를 해결해 보고, 해당 데이터셋과 모델에 대해 더 알아보면서 추가적인 전처리를 생각해본다.|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
